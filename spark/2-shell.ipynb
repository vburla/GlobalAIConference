{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "# Lab 2.2 : Spark Shell\n",
    "\n",
    "### Overview\n",
    "Get familiar with Spark shell  \n",
    "- [Standalone version](2.2-shell.ipynb)\n",
    " \n",
    "### Builds on\n",
    "[2.1-install-spark](2.1-install-spark.ipynb)\n",
    "\n",
    "### Run time\n",
    "approx. 20-30 minutes\n",
    "\n",
    "### Notes\n",
    "\n",
    "\n",
    "## STEP 1: Download Spark labs\n",
    "If you had done this already, you can go to next step.\n",
    "```bash\n",
    "    $   cd\n",
    "    $   git clone  -depth 1 git@github.com:elephantscale/spark-labs.git\n",
    "```\n",
    "\n",
    "Update the dataset\n",
    "\n",
    "```bash\n",
    "    # if  ~/data dir is missing, do the following\n",
    "    # $   git clone --depth 1  git@github.com:elephantscale/datasets.git\n",
    "\n",
    "    $   cd ~/data\n",
    "    $   git pull\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:  Using  PySpark shell\n",
    "You can run all notebook commands in PySpark shell.\n",
    "\n",
    "Start PySpark shell as follows\n",
    "\n",
    "```\n",
    "    #  go to labs directory first\n",
    "    $   cd ~/spark-labs\n",
    "    $   ~/spark/bin/pyspark  ## spark shell is in bin/ dir\n",
    "```\n",
    "\n",
    "And then you can execute the code blocks in pySpak shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Exploring Spark shell UI\n",
    "Spark shell UI is available on port 4040.\n",
    "\n",
    "In browser go to :   http://your_machine_address:4040\n",
    "(use 'public' ip of machine)\n",
    "\n",
    "Here is a sample screen shot:\n",
    "\n",
    "<img src=\"../assets/images/2a.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n",
    "**==> Explore stage, storage, environment and executor tabs**\n",
    "\n",
    "**==> Take note of 'Event Timeline', we will use this for monitoring our jobs later**\n",
    "\n",
    "**==> Check spark master on port 8080,  Do you the Spark shell application connected?  Why (not)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Spark context\n",
    "Within Spark  shell,  variable `sc` is the SparkContext.  \n",
    "Type `sc` in scala prompt and see what happens.  \n",
    "Your output might look like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# older version spark context\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# newwer Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the name of application name**\n",
    "print(sc.appName)\n",
    "\n",
    "# Find the 'Spark master' for the shell\n",
    "print(sc.master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Load a file\n",
    "We have data files under `spark-labs/data`.  \n",
    "Use test file :  `data/twinkle/sample.txt` .  \n",
    "The file has a favorite nursery rhyme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "```\n",
    "   twinkle twinkle little star\n",
    "   how I wonder what you are\n",
    "   up above the world so high\n",
    "   like a diamond in the sky\n",
    "   twinkle twinkle little star\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala "
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"../data/twinkle/sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer the following questions:\n",
    "\n",
    "**==> What is the 'type' of f ?**   \n",
    "hint : type `f` on the console\n",
    "\n",
    "**==> Inspect Spark Shell UI on port 4040, do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print the first line / record from RDD**  \n",
    "hint : `f.first()`\n",
    "\n",
    "**==> Again, inspect Spark Shell UI on port 4040, do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print first 3 lines of RDD**  \n",
    "hint : `f.take(???)`  (provide the correct argument to take function)\n",
    "\n",
    "**==> Again, inspect Spark Shell UI on port 4040, do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print all the content from the file**  \n",
    "hint : `f.collect()`\n",
    "\n",
    "**==> How many lines are in the file?**  \n",
    "hint : `f.count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO : Try the code here\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out Jobs Page\n",
    "**==> Inspect the 'Jobs' section in Shell UI (in browser)**  \n",
    "Also inspect the event time line\n",
    "\n",
    "<img src=\"../assets/images/2b.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "\n",
    "**==> Inspect the 'Executor' section in Shell UI (in browser)**\n",
    "\n",
    "<img src=\"../assets/images/2c.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "\n",
    "\n",
    "## Step 5 : Spark Session (Only in V2 and later)\n",
    "Try to load file using Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = spark.read.text(\"../data/twinkle/sample.txt\")\n",
    "# Note the type of f is RDD.  No DataSet in Pyspark\n",
    "print(f.count())\n",
    "f.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6:  Connecting Shell and  Spark server\n",
    "\n",
    "For this lab, we are going use command line pyspark\n",
    "\n",
    "### 6.1 Start Spark Server\n",
    "If Spark server is not running, start it as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "   $ ~/spark/sbin/start-all.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `jps` command to inspect the java process.  Your output might look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "```\n",
    "    731 Master\n",
    "    902 Jps\n",
    "    831 Worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark master UI is available on port 8080.\n",
    "In browser go to :   http://your_machine_address:8080\n",
    "(use 'public' ip of machine)\n",
    "\n",
    "Here is a sample screen shot:\n",
    "\n",
    "<img src=\"../assets/images/2d.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "### 6.2 Now start pyspark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "   $ ~/spark/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the shell starts, check the _server_ UI on port 8080.\n",
    "\n",
    "**==> Do you see the shell connected as an application?  why (not) ?**\n",
    "\n",
    "### 6.3 Connect Spark shell with the Spark server.\n",
    "Make a note of Spark server uri (e.g  `spark://host_name:7077`)\n",
    "\n",
    "**==> Restart spark shell as follows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash  \n",
    "  $   ~/spark/bin/pyspark   --master  spark-server-uri\n",
    "                                            ^^^^^^^^^^^^^^^^\n",
    "                                        update this to match your spark server\n",
    "\n",
    "  $   ~/spark/bin/pyspark   --master  spark://localhost:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an Amazon server you may have to use the internal ip for the spark server, such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "   ~/spark/bin/pyspark  --master spark://your_host_name:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the ES VM you may have to use the localhost.localdomain. In all cases, follow what the spark master UI tells you.\n",
    "\n",
    "\n",
    "**==> Once the shell started, check both UIs**\n",
    "\n",
    "#### spark server UI at port 8080\n",
    "\n",
    "<img src=\"../assets/images/2e.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "---\n",
    "#### spark shell UI at  port 4040\n",
    "\n",
    "<img src=\"../assets/images/2f.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "\n",
    "## STEP 7: Redo step (4) in the new shell\n",
    "Now our shell is connected to a server\n",
    "**==> Load file and test it as in Step (4)**\n",
    "\n",
    "## Tip : Dealing With Logs\n",
    "Spark Shell by default prints logs at warning (WARN) level.  If you want to change the logging level, do this at the pyspark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sc.setLogLevel(\"INFO\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to see any logs, you can start Spark shell as follows.  All the logs will be sent to 'logs' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "   $    ~/spark/bin/pyspark  2> logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Lab 1 : Start multiple Shells\n",
    "* Using one terminal, start a shell and connect to master  using **Step 5.3**\n",
    "* Using second terminal (open one if you need to), start another shell connecting to the same master\n",
    "* Check the master UI (port 8080).  You would see some thing like this, can you explain the behavior?\n",
    "\n",
    "<img src=\"../assets/images/2g.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
